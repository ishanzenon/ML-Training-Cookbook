{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introductory Machine Learning: Hands-On Overview\n",
    "\n",
    "**Welcome to Machine Learning!** 🎯\n",
    "\n",
    "This 2-hour interactive session will guide you through the fundamentals of machine learning with hands-on examples and exercises. By the end, you'll understand core ML concepts and have practical experience with real datasets.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Introduction & Setup (0-10 min)\n",
    "\n",
    "### Welcome and Goals\n",
    "\n",
    "Welcome to this introductory Machine Learning session! This notebook provides a gentle overview of ML concepts through explanations and simple interactions. Our goals are to understand ML basics, explore data, and run guided examples without overwhelming you. \n",
    "\n",
    "**Machine learning** is a subset of AI where computers learn from data to make predictions or decisions, improving over time without explicit programming.\n",
    "\n",
    "### Python Best Practices Reminder\n",
    "- Always use **descriptive variable names**\n",
    "- Add **comments** to your code\n",
    "- Handle **errors gracefully**\n",
    "\n",
    "### Virtual Environment Setup\n",
    "Before starting, initialize a virtual environment to manage dependencies:\n",
    "```bash\n",
    "# Create virtual environment\n",
    "python -m venv myenv\n",
    "\n",
    "# Activate (Unix/Mac)\n",
    "source myenv/bin/activate\n",
    "\n",
    "# Activate (Windows)\n",
    "myenv\\Scripts\\activate\n",
    "```\n",
    "\n",
    "This keeps your project isolated and reproducible.\n",
    "\n",
    "**Jupyter Tip:** Use `Shift+Enter` to run cells.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries for our ML journey\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style for better visuals\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Load the famous Iris dataset as our starting point\n",
    "iris = datasets.load_iris()\n",
    "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "iris_df['species'] = iris.target\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")\n",
    "print(f\"📊 Iris dataset loaded: {iris_df.shape[0]} samples, {iris_df.shape[1]} features\")\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session Agenda and Resources\n",
    "\n",
    "**Today's Journey:**\n",
    "1. **What is ML?** - Definitions and core concepts\n",
    "2. **Types of ML** - Supervised, unsupervised, reinforcement\n",
    "3. **ML Workflow** - End-to-end process\n",
    "4. **Case Study** - Customer churn prediction\n",
    "5. **Algorithms** - Popular methods overview\n",
    "6. **Ethics & Explainability** - Responsible AI\n",
    "7. **Next Steps** - Resources for continued learning\n",
    "\n",
    "**📚 External Resources:**\n",
    "- [Google's ML Crash Course](https://developers.google.com/machine-learning/crash-course)\n",
    "- [Fast.ai Practical Deep Learning](https://course.fast.ai/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: What Is Machine Learning? (10-25 min)\n",
    "\n",
    "### Definition and Basics\n",
    "\n",
    "**Machine learning (ML)** enables computers to learn from data and make decisions or predictions without being explicitly programmed. It differs from rule-based software by improving through experience, like a system recognizing patterns in images.\n",
    "\n",
    "**Common problems ML solves:**\n",
    "- **Prediction** (e.g., forecasting sales)\n",
    "- **Classification** (e.g., spam detection)\n",
    "- **Clustering** (grouping similar items)\n",
    "\n",
    "**Analogy:** ML is like a chef tasting soup to refine recipes over time. The more they taste (data), the better they become at creating perfect dishes (predictions).\n",
    "\n",
    "### Key Concepts:\n",
    "- **Data:** The fuel that powers ML models\n",
    "- **Features:** Individual measurable properties of observations\n",
    "- **Patterns:** Hidden relationships in data that models learn to recognize\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore our dataset to understand what ML \"sees\"\n",
    "print(\"🔍 Exploring the Iris Dataset\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Dataset shape: {iris_df.shape}\")\n",
    "print(f\"Features: {list(iris_df.columns[:-1])}\")\n",
    "print(f\"Target classes: {iris.target_names}\")\n",
    "\n",
    "# Create a scatter plot to visualize patterns\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "scatter = plt.scatter(iris_df['sepal length (cm)'], iris_df['sepal width (cm)'], \n",
    "                     c=iris_df['species'], cmap='viridis', alpha=0.7)\n",
    "plt.xlabel('Sepal Length (cm)')\n",
    "plt.ylabel('Sepal Width (cm)')\n",
    "plt.title('Sepal Measurements by Species')\n",
    "plt.colorbar(scatter)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "scatter2 = plt.scatter(iris_df['petal length (cm)'], iris_df['petal width (cm)'], \n",
    "                      c=iris_df['species'], cmap='viridis', alpha=0.7)\n",
    "plt.xlabel('Petal Length (cm)')\n",
    "plt.ylabel('Petal Width (cm)')\n",
    "plt.title('Petal Measurements by Species')\n",
    "plt.colorbar(scatter2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 Notice the patterns! Different species cluster in different regions.\")\n",
    "print(\"This is what ML algorithms learn to recognize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration Prompt\n",
    "\n",
    "Run the cell above and observe the patterns. **What do you notice?**\n",
    "- Do you see distinct groups (clusters) of points?\n",
    "- Which features seem most useful for distinguishing species?\n",
    "\n",
    "This visualization shows ML's focus on finding patterns in data.\n",
    "\n",
    "**📖 For deeper reading:** [Fast.ai ML Basics](https://course.fast.ai/Lessons/lesson1.html)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎯 Exercise 1: Data Filtering\n",
    "\n",
    "**Your Turn!** In the cell below, write 3-4 lines to:\n",
    "1. Filter the dataset to show only flowers with petal length > 4.0 cm\n",
    "2. Display the first 5 rows of the filtered data\n",
    "\n",
    "**Expected result:** You should see fewer rows, likely from specific species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here - filter dataset for petal length > 4.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>💡 Click here for solution</b></summary>\n",
    "\n",
    "```python\n",
    "# Filter for flowers with petal length > 4.0 cm\n",
    "filtered_df = iris_df[iris_df['petal length (cm)'] > 4.0]\n",
    "print(f\"Filtered dataset has {len(filtered_df)} samples\")\n",
    "filtered_df.head()\n",
    "```\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Core Types of ML (25-45 min)\n",
    "\n",
    "### Overview of ML Types\n",
    "\n",
    "ML is divided into three main categories:\n",
    "\n",
    "#### 1. **Supervised Learning** 🎯\n",
    "- **What it is:** Learning from labeled data to predict outcomes\n",
    "- **Example:** Classifying emails as spam or not spam\n",
    "- **Data:** Has both input features AND known correct answers\n",
    "\n",
    "#### 2. **Unsupervised Learning** 🔍\n",
    "- **What it is:** Finding patterns in unlabeled data\n",
    "- **Example:** Customer segmentation for marketing\n",
    "- **Data:** Only input features, no \"correct\" answers\n",
    "\n",
    "#### 3. **Reinforcement Learning** 🎮\n",
    "- **What it is:** Learning through trial-and-error feedback\n",
    "- **Example:** Training a game AI or autonomous vehicle\n",
    "- **Data:** Actions and rewards/penalties\n",
    "\n",
    "**Key Insight:** The type of learning depends on your data and problem!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's demonstrate supervised vs unsupervised learning\n",
    "print(\"🎯 SUPERVISED LEARNING DEMO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Prepare data for supervised learning\n",
    "X = iris_df[['sepal length (cm)', 'sepal width (cm)']].values  # Features\n",
    "y = iris_df['species'].values  # Labels (what we want to predict)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a supervised model (Logistic Regression)\n",
    "supervised_model = LogisticRegression()\n",
    "supervised_model.fit(X_train, y_train)  # Learn from labeled training data\n",
    "\n",
    "# Make predictions on test data\n",
    "predictions = supervised_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "print(f\"Supervised model accuracy: {accuracy:.2%}\")\n",
    "print(f\"The model correctly predicted {accuracy:.1%} of species!\")\n",
    "\n",
    "print(\"\\n🔍 UNSUPERVISED LEARNING DEMO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Unsupervised learning: Find clusters without knowing species labels\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)  # We suspect 3 groups\n",
    "clusters = kmeans.fit_predict(X)  # Find natural groupings\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Supervised learning visualization\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='viridis', alpha=0.7)\n",
    "plt.title('Supervised Learning\\n(True Species Labels)')\n",
    "plt.xlabel('Sepal Length')\n",
    "plt.ylabel('Sepal Width')\n",
    "\n",
    "# Unsupervised learning visualization\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=clusters, cmap='viridis', alpha=0.7)\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], \n",
    "           marker='x', s=200, linewidths=3, color='red')\n",
    "plt.title('Unsupervised Learning\\n(Discovered Clusters)')\n",
    "plt.xlabel('Sepal Length')\n",
    "plt.ylabel('Sepal Width')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 Compare the two plots:\")\n",
    "print(\"   Left: Uses known labels (supervised)\")\n",
    "print(\"   Right: Discovers patterns without labels (unsupervised)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guided Questions\n",
    "\n",
    "**Reflection Time:** *(Allocate 2-3 min for discussion)*\n",
    "- How do the clusters compare to the true species labels?\n",
    "- What's the key difference between having labeled vs. unlabeled data?\n",
    "- When might you use each approach?\n",
    "\n",
    "**📖 For hands-on examples:** [Towards Data Science - ML Types](https://towardsdatascience.com/5-types-of-machine-learning-algorithms-you-need-to-know-5ac7fce8920d/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎯 Exercise 2: Clustering Experiment\n",
    "\n",
    "**Your Turn!** In the cell below, write 2-3 lines to:\n",
    "1. Change the number of clusters in k-means to 4\n",
    "2. Fit the model and get new cluster labels\n",
    "3. Print the unique cluster labels\n",
    "\n",
    "**Expected result:** You should see labels 0, 1, 2, 3 (four clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here - create 4 clusters and print the labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>💡 Click here for solution</b></summary>\n",
    "\n",
    "```python\n",
    "# Create k-means with 4 clusters\n",
    "kmeans_4 = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "labels_4 = kmeans_4.fit_predict(X)\n",
    "print(f\"Unique cluster labels: {sorted(np.unique(labels_4))}\")\n",
    "print(f\"Number of points in each cluster: {np.bincount(labels_4)}\")\n",
    "```\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: End-to-End ML Workflow (45-60 min)\n",
    "\n",
    "### The 6-Step ML Process\n",
    "\n",
    "Every ML project follows a similar workflow:\n",
    "\n",
    "```\n",
    "1. 🎯 Define Problem → 2. 📊 Collect Data → 3. 🔧 Prepare Features → \n",
    "4. 🤖 Train Model → 5. 📈 Evaluate Performance → 6. 🚀 Deploy\n",
    "```\n",
    "\n",
    "### Today's Use Case: House Price Prediction\n",
    "\n",
    "**Problem:** Predict house prices based on features like size, location, and age\n",
    "**Type:** Regression (predicting continuous values)\n",
    "**Dataset:** Boston Housing (classic ML dataset)\n",
    "\n",
    "### Key Principles:\n",
    "- **Data Quality Matters:** Clean data leads to reliable models\n",
    "- **Feature Engineering:** Selecting the right features is crucial\n",
    "- **Iterative Process:** ML requires experimentation and refinement\n",
    "\n",
    "**⚠️ Important:** Always check for bias in your data to ensure fair predictions!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed version replacing load_boston with fetch_california_housing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"🏠 Section 3: Regression Example - Predicting Housing Prices\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Step 1: Load the California Housing dataset (replacement for Boston Housing)\n",
    "housing = fetch_california_housing()\n",
    "housing_df = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "housing_df['PRICE'] = housing.target  # Target variable (house prices in hundreds of thousands)\n",
    "\n",
    "print(f\"📊 Step 1 - Data Loaded: {housing_df.shape[0]} houses, {housing_df.shape[1]-1} features\")\n",
    "print(f\"   Features include: {list(housing.feature_names[:5])}...\")\n",
    "print(f\"   Price range: ${housing_df['PRICE'].min():.1f}00k - ${housing_df['PRICE'].max():.1f}00k\")\n",
    "\n",
    "# Step 2: Data Quality Check\n",
    "print(f\"\\n🔍 Step 2 - Data Quality Check:\")\n",
    "print(f\"   Missing values: {housing_df.isnull().sum().sum()}\")\n",
    "print(f\"   Data types: All numeric ✅\")\n",
    "\n",
    "# Step 3: Feature Selection and Preparation\n",
    "# Select a few key features for simplicity\n",
    "features = ['MedInc', 'HouseAge', 'AveRooms']  # Median income, house age, average rooms\n",
    "X = housing_df[features].values\n",
    "y = housing_df['PRICE'].values\n",
    "\n",
    "print(f\"\\n🔧 Step 3 - Feature Selection:\")\n",
    "print(f\"   Selected features: {features}\")\n",
    "print(f\"   MedInc: Median income in block group\")\n",
    "print(f\"   HouseAge: Median house age in block group\")\n",
    "print(f\"   AveRooms: Average number of rooms per household\")\n",
    "\n",
    "# Step 4: Split data and train model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)  # This is where the \"learning\" happens!\n",
    "\n",
    "print(f\"\\n🤖 Step 4 - Model Training:\")\n",
    "print(f\"   Algorithm: Linear Regression\")\n",
    "print(f\"   Training samples: {len(X_train)}\")\n",
    "print(f\"   Model coefficients learned: {model.coef_.round(2)}\")\n",
    "\n",
    "# Step 5: Evaluate performance\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n📈 Step 5 - Model Performance:\")\n",
    "print(f\"   R² Score: {r2:.3f} (higher is better, max = 1.0)\")\n",
    "print(f\"   Mean Squared Error: {mse:.3f}\")\n",
    "print(f\"   This means our model explains {r2*100:.1f}% of price variation!\")\n",
    "\n",
    "# Step 6: Make a prediction\n",
    "print(f\"\\n🔮 Step 6 - Making Predictions:\")\n",
    "sample_house = [[8.0, 10.0, 6.0]]  # High income, 10-year-old house, 6 rooms avg\n",
    "predicted_price = model.predict(sample_house)[0]\n",
    "print(f\"   For a house with features {sample_house[0]}:\")\n",
    "print(f\"   Predicted price: ${predicted_price:.1f}00,000\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Prices')\n",
    "plt.ylabel('Predicted Prices')\n",
    "plt.title('California Housing: Actual vs Predicted Prices')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✅ Regression Complete! The model learned to predict house prices based on key features.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Prompt\n",
    "\n",
    "**Experiment Time:** Try modifying the code above:\n",
    "- Remove the 'PTRATIO' feature and re-run\n",
    "- How does the accuracy change?\n",
    "- What does this tell you about feature importance?\n",
    "\n",
    "This highlights the **iterative nature** of ML workflows.\n",
    "\n",
    "**📖 Learn more:** [IBM - ML Pipeline](https://www.ibm.com/think/topics/machine-learning-pipeline)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎯 Exercise 3: Build Your Own Predictor\n",
    "\n",
    "**Your Turn!** In the cell below, write 3-5 lines to:\n",
    "1. Select only two features: 'RM' and 'LSTAT'\n",
    "2. Train a linear regression model on these features\n",
    "3. Predict the price for a house with 7 rooms and 5% lower status\n",
    "\n",
    "**Expected result:** A price prediction in the reasonable range ($20-50k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here - build a 2-feature model and make a prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>💡 Click here for solution</b></summary>\n",
    "\n",
    "```python\n",
    "# Select two features and train model\n",
    "X_simple = boston_df[['RM', 'LSTAT']].values\n",
    "simple_model = LinearRegression()\n",
    "simple_model.fit(X_simple, y)\n",
    "\n",
    "# Make prediction for house with 7 rooms, 5% lower status\n",
    "prediction = simple_model.predict([[7.0, 5.0]])[0]\n",
    "print(f\"Predicted price: ${prediction:.1f}k\")\n",
    "```\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Mini Case Study - Customer Churn Prediction (60-80 min)\n",
    "\n",
    "### Real-World Scenario: Telecom Customer Retention\n",
    "\n",
    "**Business Problem:** A telecom company wants to identify customers likely to cancel their service (\"churn\") so they can proactively offer retention incentives.\n",
    "\n",
    "**ML Approach:** \n",
    "- **Type:** Classification (Will customer churn? Yes/No)\n",
    "- **Features:** Usage patterns, tenure, demographics, service details\n",
    "- **Goal:** Predict churn probability for each customer\n",
    "\n",
    "### Key ML Concepts:\n",
    "- **Binary Classification:** Predicting one of two outcomes\n",
    "- **Confusion Matrix:** Shows correct vs. incorrect predictions\n",
    "- **Business Impact:** ML predictions drive real business decisions\n",
    "\n",
    "**Story Context:** Imagine you're a data scientist helping this company save millions by retaining valuable customers!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic customer churn dataset for demonstration\n",
    "print(\"📞 CUSTOMER CHURN PREDICTION CASE STUDY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Generate synthetic customer data\n",
    "np.random.seed(42)\n",
    "n_customers = 1000\n",
    "\n",
    "# Create realistic customer features\n",
    "tenure_months = np.random.normal(24, 12, n_customers).clip(1, 60)  # 1-60 months\n",
    "monthly_charges = np.random.normal(65, 20, n_customers).clip(20, 120)  # $20-120\n",
    "total_charges = tenure_months * monthly_charges + np.random.normal(0, 100, n_customers)\n",
    "age = np.random.normal(45, 15, n_customers).clip(18, 80)  # 18-80 years\n",
    "support_calls = np.random.poisson(2, n_customers).clip(0, 10)  # 0-10 calls\n",
    "\n",
    "# Create churn labels based on realistic patterns\n",
    "# Higher churn probability for: short tenure, high charges, many support calls\n",
    "churn_probability = (\n",
    "    0.1 +  # Base churn rate\n",
    "    0.4 * (tenure_months < 12) +  # New customers more likely to churn\n",
    "    0.3 * (monthly_charges > 80) +  # High charges increase churn\n",
    "    0.2 * (support_calls > 3)  # Many support calls indicate problems\n",
    ").clip(0, 0.9)\n",
    "\n",
    "churn = np.random.binomial(1, churn_probability, n_customers)\n",
    "\n",
    "# Create DataFrame\n",
    "churn_df = pd.DataFrame({\n",
    "    'tenure_months': tenure_months,\n",
    "    'monthly_charges': monthly_charges,\n",
    "    'total_charges': total_charges,\n",
    "    'age': age,\n",
    "    'support_calls': support_calls,\n",
    "    'churned': churn\n",
    "})\n",
    "\n",
    "print(f\"📊 Dataset Created: {len(churn_df)} customers\")\n",
    "print(f\"   Churn rate: {churn_df['churned'].mean():.1%}\")\n",
    "print(f\"   Features: {list(churn_df.columns[:-1])}\")\n",
    "\n",
    "# Explore the data\n",
    "print(\"\\n🔍 Customer Profile Summary:\")\n",
    "print(churn_df.describe().round(1))\n",
    "\n",
    "# Visualize churn patterns\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Tenure vs Churn\n",
    "axes[0,0].boxplot([churn_df[churn_df['churned']==0]['tenure_months'],\n",
    "                   churn_df[churn_df['churned']==1]['tenure_months']], \n",
    "                  labels=['Stayed', 'Churned'])\n",
    "axes[0,0].set_title('Tenure by Churn Status')\n",
    "axes[0,0].set_ylabel('Tenure (months)')\n",
    "\n",
    "# Monthly Charges vs Churn\n",
    "axes[0,1].boxplot([churn_df[churn_df['churned']==0]['monthly_charges'],\n",
    "                   churn_df[churn_df['churned']==1]['monthly_charges']], \n",
    "                  labels=['Stayed', 'Churned'])\n",
    "axes[0,1].set_title('Monthly Charges by Churn Status')\n",
    "axes[0,1].set_ylabel('Monthly Charges ($)')\n",
    "\n",
    "# Support Calls vs Churn\n",
    "axes[1,0].boxplot([churn_df[churn_df['churned']==0]['support_calls'],\n",
    "                   churn_df[churn_df['churned']==1]['support_calls']], \n",
    "                  labels=['Stayed', 'Churned'])\n",
    "axes[1,0].set_title('Support Calls by Churn Status')\n",
    "axes[1,0].set_ylabel('Number of Support Calls')\n",
    "\n",
    "# Churn Rate by Tenure Groups\n",
    "churn_df['tenure_group'] = pd.cut(churn_df['tenure_months'], \n",
    "                                  bins=[0, 12, 24, 36, 60], \n",
    "                                  labels=['0-12m', '12-24m', '24-36m', '36-60m'])\n",
    "churn_by_tenure = churn_df.groupby('tenure_group')['churned'].mean()\n",
    "axes[1,1].bar(range(len(churn_by_tenure)), churn_by_tenure.values)\n",
    "axes[1,1].set_title('Churn Rate by Tenure Group')\n",
    "axes[1,1].set_ylabel('Churn Rate')\n",
    "axes[1,1].set_xticks(range(len(churn_by_tenure)))\n",
    "axes[1,1].set_xticklabels(churn_by_tenure.index)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 Pattern Insights:\")\n",
    "print(\"   📈 Newer customers (low tenure) churn more\")\n",
    "print(\"   💰 Higher monthly charges correlate with churn\")\n",
    "print(\"   📞 More support calls often indicate dissatisfaction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and evaluate the churn prediction model\n",
    "print(\"🤖 BUILDING THE CHURN PREDICTION MODEL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Prepare features and target\n",
    "feature_columns = ['tenure_months', 'monthly_charges', 'total_charges', 'age', 'support_calls']\n",
    "X = churn_df[feature_columns].values\n",
    "y = churn_df['churned'].values\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a decision tree classifier (good for interpretability)\n",
    "churn_model = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "churn_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = churn_model.predict(X_test)\n",
    "y_pred_proba = churn_model.predict_proba(X_test)[:, 1]  # Probability of churning\n",
    "\n",
    "# Evaluate model performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"📊 Model Performance:\")\n",
    "print(f\"   Accuracy: {accuracy:.1%}\")\n",
    "print(f\"   Training samples: {len(X_train)}\")\n",
    "print(f\"   Test samples: {len(X_test)}\")\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "           xticklabels=['Stayed', 'Churned'], \n",
    "           yticklabels=['Stayed', 'Churned'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "# Feature Importance\n",
    "plt.subplot(1, 2, 2)\n",
    "importance = churn_model.feature_importances_\n",
    "feature_names = feature_columns\n",
    "plt.bar(range(len(importance)), importance)\n",
    "plt.title('Feature Importance for Churn Prediction')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.xticks(range(len(feature_names)), feature_names, rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Business interpretation\n",
    "print(f\"\\n🎯 Confusion Matrix Interpretation:\")\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "print(f\"   ✅ Correctly predicted to stay: {tn}\")\n",
    "print(f\"   ✅ Correctly predicted to churn: {tp}\")\n",
    "print(f\"   ❌ False alarms (predicted churn, but stayed): {fp}\")\n",
    "print(f\"   ❌ Missed churners (predicted stay, but churned): {fn}\")\n",
    "\n",
    "print(f\"\\n💼 Business Impact:\")\n",
    "print(f\"   📈 Model identifies {tp} real churners out of {tp + fn} total\")\n",
    "print(f\"   💰 Could potentially save ${tp * 65 * 12:,.0f} in annual revenue\")\n",
    "print(f\"   ⚠️  Would waste retention efforts on {fp} false alarms\")\n",
    "\n",
    "# Demo: Predict for new customers\n",
    "print(f\"\\n🔮 Example Predictions:\")\n",
    "new_customers = [\n",
    "    [6, 90, 540, 35, 5],   # High risk: short tenure, high charges, many calls\n",
    "    [36, 45, 1620, 50, 1], # Low risk: long tenure, reasonable charges, few calls\n",
    "]\n",
    "\n",
    "for i, customer in enumerate(new_customers, 1):\n",
    "    churn_prob = churn_model.predict_proba([customer])[0, 1]\n",
    "    risk_level = \"HIGH\" if churn_prob > 0.5 else \"LOW\"\n",
    "    print(f\"   Customer {i}: {churn_prob:.1%} churn probability ({risk_level} risk)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poll and Takeaways\n",
    "\n",
    "**Quick Poll:** *(Allocate 2-3 min for discussion)*\n",
    "- Which feature do you think matters most for predicting churn?\n",
    "- How might this company use these predictions?\n",
    "- What are the risks of false positives vs. false negatives?\n",
    "\n",
    "**Key Takeaways:**\n",
    "- ML transforms business data into actionable insights\n",
    "- Model interpretation is crucial for business decisions\n",
    "- Understanding prediction errors helps optimize strategies\n",
    "\n",
    "**📖 Try similar projects:** [Kaggle - Customer Churn](https://www.kaggle.com/blastchar/telco-customer-churn)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎯 Exercise 4: Data Splitting Practice\n",
    "\n",
    "**Your Turn!** In the cell below, write 4-5 lines to:\n",
    "1. Load a subset of the churn dataset (first 500 customers)\n",
    "2. Split it into 80% training and 20% testing\n",
    "3. Print the size of both training and testing sets\n",
    "\n",
    "**Expected result:** Training set ~400 samples, testing set ~100 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here - split the dataset and show sizes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>💡 Click here for solution</b></summary>\n",
    "\n",
    "```python\n",
    "# Load subset and split data\n",
    "subset_df = churn_df.head(500)\n",
    "X_subset = subset_df[feature_columns].values\n",
    "y_subset = subset_df['churned'].values\n",
    "X_train_sub, X_test_sub, y_train_sub, y_test_sub = train_test_split(\n",
    "    X_subset, y_subset, test_size=0.2, random_state=42)\n",
    "print(f\"Training set size: {len(X_train_sub)}\")\n",
    "print(f\"Testing set size: {len(X_test_sub)}\")\n",
    "```\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Popular Algorithms - Quick Tour (80-95 min)\n",
    "\n",
    "### High-Level Algorithm Overview\n",
    "\n",
    "ML has many algorithms, each suited for different problems. Here are the most common ones:\n",
    "\n",
    "#### 🔢 **Linear Regression**\n",
    "- **Use:** Predicts continuous values (prices, temperatures)\n",
    "- **How it works:** Finds the best line through data points\n",
    "- **Example:** House price prediction\n",
    "\n",
    "#### 🌳 **Decision Trees**\n",
    "- **Use:** Classification and regression\n",
    "- **How it works:** Makes decisions like a flowchart\n",
    "- **Example:** \"If age > 30 AND income > 50k, then approve loan\"\n",
    "\n",
    "#### 🎯 **k-Means Clustering**\n",
    "- **Use:** Groups similar data points\n",
    "- **How it works:** Finds cluster centers and assigns points\n",
    "- **Example:** Customer segmentation\n",
    "\n",
    "#### 🧠 **Neural Networks**\n",
    "- **Use:** Complex pattern recognition\n",
    "- **How it works:** Mimics brain neurons in layers\n",
    "- **Example:** Image recognition, language processing\n",
    "\n",
    "**Remember:** No single algorithm is \"best\" - it depends on your data and problem!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick demonstration of different algorithms\n",
    "print(\"🧠 ALGORITHM COMPARISON DEMO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Use Iris dataset for comparison\n",
    "X_iris = iris_df[['sepal length (cm)', 'sepal width (cm)']].values\n",
    "y_iris = iris_df['species'].values\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_iris, y_iris, test_size=0.3, random_state=42)\n",
    "\n",
    "# Test different algorithms\n",
    "algorithms = {\n",
    "    'Decision Tree (depth=3)': DecisionTreeClassifier(max_depth=3, random_state=42),\n",
    "    'Decision Tree (depth=5)': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "    'Decision Tree (depth=10)': DecisionTreeClassifier(max_depth=10, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42)\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, algorithm in algorithms.items():\n",
    "    # Train the algorithm\n",
    "    algorithm.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate performance\n",
    "    train_accuracy = algorithm.score(X_train, y_train)\n",
    "    test_accuracy = algorithm.score(X_test, y_test)\n",
    "    \n",
    "    results.append({\n",
    "        'Algorithm': name,\n",
    "        'Train Accuracy': f\"{train_accuracy:.1%}\",\n",
    "        'Test Accuracy': f\"{test_accuracy:.1%}\"\n",
    "    })\n",
    "    \n",
    "    print(f\"✅ {name}:\")\n",
    "    print(f\"   Training accuracy: {train_accuracy:.1%}\")\n",
    "    print(f\"   Testing accuracy: {test_accuracy:.1%}\")\n",
    "    \n",
    "    # Check for overfitting\n",
    "    if train_accuracy - test_accuracy > 0.1:\n",
    "        print(f\"   ⚠️  Possible overfitting detected!\")\n",
    "    print()\n",
    "\n",
    "# Visualize decision tree example\n",
    "tree_model = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Data points\n",
    "plt.subplot(1, 2, 1)\n",
    "scatter = plt.scatter(X_iris[:, 0], X_iris[:, 1], c=y_iris, cmap='viridis', alpha=0.7)\n",
    "plt.xlabel('Sepal Length (cm)')\n",
    "plt.ylabel('Sepal Width (cm)')\n",
    "plt.title('Iris Dataset')\n",
    "plt.colorbar(scatter)\n",
    "\n",
    "# Plot 2: Algorithm comparison\n",
    "plt.subplot(1, 2, 2)\n",
    "algorithms_short = ['Tree (3)', 'Tree (5)', 'Tree (10)', 'Logistic']\n",
    "test_scores = [float(r['Test Accuracy'].strip('%'))/100 for r in results]\n",
    "bars = plt.bar(algorithms_short, test_scores, color=['skyblue', 'lightgreen', 'salmon', 'gold'])\n",
    "plt.title('Algorithm Performance Comparison')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars, test_scores):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{score:.1%}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"💡 Key Observations:\")\n",
    "print(\"   🌳 Decision tree depth affects complexity and overfitting\")\n",
    "print(\"   📊 Different algorithms can achieve similar performance\")\n",
    "print(\"   ⚖️  Balance between model complexity and generalization\")\n",
    "\n",
    "# Demo feature importance\n",
    "print(f\"\\n🎯 Decision Tree Feature Importance:\")\n",
    "feature_names = ['Sepal Length', 'Sepal Width']\n",
    "importance = tree_model.feature_importances_\n",
    "for name, imp in zip(feature_names, importance):\n",
    "    print(f\"   {name}: {imp:.3f}\")\n",
    "\n",
    "most_important = feature_names[np.argmax(importance)]\n",
    "print(f\"\\n   🏆 Most important feature: {most_important}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Prompt\n",
    "\n",
    "**Try This:** Go back to the code above and:\n",
    "- Change the tree depth to 1 or 2\n",
    "- Re-run the comparison\n",
    "- What happens to performance?\n",
    "\n",
    "This demonstrates the **bias-variance tradeoff** - simpler models may underfit, complex models may overfit.\n",
    "\n",
    "**📖 Explore more algorithms:** [Machine Learning Mastery - Algorithm Guide](https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎯 Exercise 5: Algorithm Tuning\n",
    "\n",
    "**Your Turn!** In the cell below, write 2-3 lines to:\n",
    "1. Create a decision tree with max_depth=2\n",
    "2. Fit it to the iris training data\n",
    "3. Print its accuracy score on the training data\n",
    "\n",
    "**Expected result:** Accuracy between 0.8 and 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here - create shallow tree and check accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>💡 Click here for solution</b></summary>\n",
    "\n",
    "```python\n",
    "# Create and train shallow decision tree\n",
    "shallow_tree = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "shallow_tree.fit(X_train, y_train)\n",
    "accuracy = shallow_tree.score(X_train, y_train)\n",
    "print(f\"Shallow tree accuracy: {accuracy:.1%}\")\n",
    "```\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Ethics, Explainability & Responsible AI (95-110 min)\n",
    "\n",
    "### Key Ethical Concepts in ML\n",
    "\n",
    "As ML becomes more powerful, we must consider its impact on society:\n",
    "\n",
    "#### ⚖️ **Bias and Fairness**\n",
    "- **Problem:** Biased training data leads to unfair decisions\n",
    "- **Example:** Facial recognition working poorly for certain ethnic groups\n",
    "- **Solution:** Diverse datasets, bias testing, fairness metrics\n",
    "\n",
    "#### 🔍 **Explainability vs. Interpretability**\n",
    "- **Explainability:** Understanding why a model made a specific prediction (post-hoc)\n",
    "- **Interpretability:** Models that are transparent by design (e.g., decision trees)\n",
    "- **Importance:** Builds trust, enables debugging, meets regulatory requirements\n",
    "\n",
    "#### 🔒 **Privacy and Security**\n",
    "- **Concerns:** Protecting sensitive data, preventing model attacks\n",
    "- **Methods:** Differential privacy, federated learning, secure aggregation\n",
    "\n",
    "#### 📋 **Accountability**\n",
    "- **Questions:** Who is responsible for ML decisions? How do we audit systems?\n",
    "- **Approaches:** Clear documentation, human oversight, regular audits\n",
    "\n",
    "**Golden Rule:** Always ask \"Who might be harmed by this model, and how can we prevent it?\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate model explainability and bias detection\n",
    "print(\"🔍 MODEL EXPLAINABILITY & BIAS ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Use our churn model for explainability demo\n",
    "feature_names = ['tenure_months', 'monthly_charges', 'total_charges', 'age', 'support_calls']\n",
    "\n",
    "print(\"📊 Feature Importance Analysis:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Get feature importance from our trained churn model\n",
    "importance_scores = churn_model.feature_importances_\n",
    "feature_importance = list(zip(feature_names, importance_scores))\n",
    "feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top factors influencing churn predictions:\")\n",
    "for i, (feature, importance) in enumerate(feature_importance, 1):\n",
    "    print(f\"   {i}. {feature.replace('_', ' ').title()}: {importance:.3f}\")\n",
    "    \n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Feature importance plot\n",
    "plt.subplot(2, 2, 1)\n",
    "features_sorted = [f[0] for f in feature_importance]\n",
    "importance_sorted = [f[1] for f in feature_importance]\n",
    "plt.barh(features_sorted, importance_sorted, color='skyblue')\n",
    "plt.title('Feature Importance in Churn Model')\n",
    "plt.xlabel('Importance Score')\n",
    "\n",
    "# Bias analysis: Check predictions by age groups\n",
    "plt.subplot(2, 2, 2)\n",
    "churn_df['age_group'] = pd.cut(churn_df['age'], bins=[0, 30, 50, 100], labels=['Young', 'Middle', 'Senior'])\n",
    "age_churn_rates = churn_df.groupby('age_group')['churned'].mean()\n",
    "plt.bar(age_churn_rates.index, age_churn_rates.values, color='lightcoral')\n",
    "plt.title('Churn Rate by Age Group')\n",
    "plt.ylabel('Churn Rate')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Model predictions distribution\n",
    "plt.subplot(2, 2, 3)\n",
    "predictions_proba = churn_model.predict_proba(X)[:, 1]\n",
    "plt.hist(predictions_proba, bins=20, alpha=0.7, color='lightgreen')\n",
    "plt.title('Distribution of Churn Probabilities')\n",
    "plt.xlabel('Predicted Churn Probability')\n",
    "plt.ylabel('Number of Customers')\n",
    "\n",
    "# Decision boundary visualization\n",
    "plt.subplot(2, 2, 4)\n",
    "# Focus on two key features for visualization\n",
    "scatter = plt.scatter(churn_df['tenure_months'], churn_df['monthly_charges'], \n",
    "                     c=predictions_proba, cmap='RdYlBu_r', alpha=0.6)\n",
    "plt.xlabel('Tenure (months)')\n",
    "plt.ylabel('Monthly Charges ($)')\n",
    "plt.title('Churn Risk Heatmap')\n",
    "plt.colorbar(scatter, label='Churn Probability')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Bias detection analysis\n",
    "print(f\"\\n⚖️ Bias Analysis Results:\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "for age_group in age_churn_rates.index:\n",
    "    rate = age_churn_rates[age_group]\n",
    "    overall_rate = churn_df['churned'].mean()\n",
    "    bias_ratio = rate / overall_rate\n",
    "    \n",
    "    if bias_ratio > 1.2:\n",
    "        status = \"⚠️  Higher risk group\"\n",
    "    elif bias_ratio < 0.8:\n",
    "        status = \"✅ Lower risk group\"\n",
    "    else:\n",
    "        status = \"✅ Similar to average\"\n",
    "    \n",
    "    print(f\"   {age_group} customers: {rate:.1%} churn rate {status}\")\n",
    "\n",
    "print(f\"\\n🎯 Explainability Insights:\")\n",
    "print(\"=\" * 25)\n",
    "top_feature = feature_importance[0][0].replace('_', ' ').title()\n",
    "print(f\"   🏆 Most predictive factor: {top_feature}\")\n",
    "print(f\"   📈 Model focuses on customer behavior patterns\")\n",
    "print(f\"   🔍 Clear business logic: unhappy customers call support more\")\n",
    "\n",
    "# Example of individual prediction explanation\n",
    "print(f\"\\n🔮 Individual Prediction Explanation:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Take a specific customer example\n",
    "customer_idx = 42\n",
    "customer_data = X[customer_idx]\n",
    "customer_prediction = churn_model.predict_proba([customer_data])[0, 1]\n",
    "\n",
    "print(f\"Customer Profile:\")\n",
    "for feature, value in zip(feature_names, customer_data):\n",
    "    print(f\"   {feature.replace('_', ' ').title()}: {value:.1f}\")\n",
    "\n",
    "print(f\"\\nPredicted churn probability: {customer_prediction:.1%}\")\n",
    "risk_level = \"HIGH\" if customer_prediction > 0.5 else \"MEDIUM\" if customer_prediction > 0.3 else \"LOW\"\n",
    "print(f\"Risk level: {risk_level}\")\n",
    "\n",
    "# Simple explanation based on feature importance\n",
    "print(f\"\\nKey factors for this prediction:\")\n",
    "for feature, value in zip(feature_names, customer_data):\n",
    "    feature_idx = feature_names.index(feature)\n",
    "    contribution = importance_scores[feature_idx] * (value / np.mean(X[:, feature_idx]))\n",
    "    if contribution > 0.1:\n",
    "        print(f\"   📈 {feature.replace('_', ' ').title()}: Increases churn risk\")\n",
    "    elif contribution < -0.1:\n",
    "        print(f\"   📉 {feature.replace('_', ' ').title()}: Decreases churn risk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection and Discussion\n",
    "\n",
    "**Critical Questions:** *(Allocate 3-4 min for discussion)*\n",
    "\n",
    "1. **Bias Detection:** Did you notice any concerning patterns in the age group analysis?\n",
    "2. **Fairness:** Should all age groups have the same churn rates? Why or why not?\n",
    "3. **Transparency:** How important is it for customers to understand why they're flagged as \"high churn risk\"?\n",
    "4. **Business Ethics:** What if the model is wrong about a loyal customer?\n",
    "\n",
    "### Real-World Considerations:\n",
    "- **Healthcare:** Biased diagnostic tools can worsen health disparities\n",
    "- **Hiring:** Unfair recruiting algorithms can perpetuate workplace inequality\n",
    "- **Criminal Justice:** Biased risk assessment tools can lead to unjust sentencing\n",
    "- **Finance:** Discriminatory loan approval can deny opportunities\n",
    "\n",
    "**📖 Deep dive into ML ethics:** [MIT Technology Review - AI Ethics](https://www.technologyreview.com/2024/02/15/1087815/responsible-technology-use-in-the-ai-age/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎯 Exercise 6: Feature Importance Analysis\n",
    "\n",
    "**Your Turn!** In the cell below, write 3-4 lines to:\n",
    "1. Extract feature importances from our churn model\n",
    "2. Sort them in descending order\n",
    "3. Print the top 3 most important features with their scores\n",
    "\n",
    "**Expected result:** A ranked list of the 3 most predictive features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here - extract and rank feature importances\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>💡 Click here for solution</b></summary>\n",
    "\n",
    "```python\n",
    "# Extract and sort feature importances\n",
    "importances = churn_model.feature_importances_\n",
    "feature_rankings = sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top 3 most important features:\")\n",
    "for i, (feature, importance) in enumerate(feature_rankings[:3], 1):\n",
    "    print(f\"{i}. {feature}: {importance:.3f}\")\n",
    "```\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Wrap-Up & Next Steps (110-120 min)\n",
    "\n",
    "### 🎯 Key Takeaways\n",
    "\n",
    "**Congratulations!** You've completed your introduction to machine learning. Here's what you've learned:\n",
    "\n",
    "#### Core Concepts:\n",
    "- **ML Definition:** Computers learning from data to make predictions without explicit programming\n",
    "- **Three Types:** Supervised (labeled data), Unsupervised (patterns), Reinforcement (trial-and-error)\n",
    "- **Workflow:** Problem → Data → Features → Model → Evaluate → Deploy\n",
    "- **Ethics:** Ensure fairness, explainability, and accountability in AI systems\n",
    "\n",
    "#### Practical Skills:\n",
    "- ✅ Loading and exploring datasets\n",
    "- ✅ Training classification and regression models\n",
    "- ✅ Evaluating model performance\n",
    "- ✅ Interpreting feature importance\n",
    "- ✅ Detecting potential bias\n",
    "\n",
    "#### Business Applications:\n",
    "- 🏠 **Real Estate:** Predicting house prices\n",
    "- 📞 **Telecom:** Customer churn prevention\n",
    "- 🌸 **Biology:** Species classification\n",
    "- 🎯 **Marketing:** Customer segmentation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚀 Continue Your ML Journey\n",
    "\n",
    "#### **Free Learning Resources:**\n",
    "\n",
    "**📚 Courses:**\n",
    "- [freeCodeCamp ML Course](https://www.freecodecamp.org/learn/machine-learning-with-python/) - Comprehensive hands-on curriculum\n",
    "- [Coursera - Machine Learning by Andrew Ng](https://www.coursera.org/learn/machine-learning) - Classic foundational course\n",
    "- [Fast.ai Practical Deep Learning](https://course.fast.ai/) - Top-down, practical approach\n",
    "\n",
    "**🛠️ Practice Platforms:**\n",
    "- [Kaggle Learn](https://www.kaggle.com/learn) - Micro-courses with datasets\n",
    "- [Google Colab](https://colab.research.google.com/) - Free Jupyter notebooks in the cloud\n",
    "- [Papers With Code](https://paperswithcode.com/) - Latest research with implementation\n",
    "\n",
    "**📖 Documentation:**\n",
    "- [Scikit-learn User Guide](https://scikit-learn.org/stable/user_guide.html) - Comprehensive ML library docs\n",
    "- [Pandas Documentation](https://pandas.pydata.org/docs/) - Data manipulation essentials\n",
    "- [Matplotlib Tutorials](https://matplotlib.org/stable/tutorials/index.html) - Data visualization\n",
    "\n",
    "#### **Next Steps Roadmap:**\n",
    "\n",
    "**Week 1-2:** Practice with Kaggle competitions\n",
    "**Week 3-4:** Build your first end-to-end project\n",
    "**Month 2:** Explore deep learning with TensorFlow/PyTorch\n",
    "**Month 3:** Specialize in your area of interest (NLP, Computer Vision, etc.)\n",
    "\n",
    "#### **Project Ideas to Try:**\n",
    "- 🎬 Movie recommendation system\n",
    "- 📈 Stock price prediction\n",
    "- 🗣️ Sentiment analysis of social media\n",
    "- 🖼️ Image classification with your own photos\n",
    "- 🏥 Healthcare data analysis\n",
    "\n",
    "---\n",
    "\n",
    "### 🤝 Q&A Session\n",
    "\n",
    "**Your Questions Welcome!** *(Remaining time for discussion)*\n",
    "\n",
    "Common questions we can address:\n",
    "- How do I choose the right algorithm for my problem?\n",
    "- What's the difference between AI, ML, and deep learning?\n",
    "- How much math do I need to know for ML?\n",
    "- What programming skills should I develop next?\n",
    "- How can I apply ML to my current work?\n",
    "- What are the career opportunities in ML?\n",
    "\n",
    "---\n",
    "\n",
    "### 🎊 Thank You!\n",
    "\n",
    "**You've taken your first step into the exciting world of machine learning!** \n",
    "\n",
    "Remember:\n",
    "- Start with simple projects and gradually increase complexity\n",
    "- Focus on understanding the problem before choosing algorithms\n",
    "- Always consider the ethical implications of your models\n",
    "- The ML community is welcoming - don't hesitate to ask questions!\n",
    "\n",
    "**Happy learning, and welcome to the ML community!** 🚀🤖\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook was designed to be your launching pad into machine learning. Bookmark it, refer back to it, and most importantly - keep experimenting!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
